{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in more details\n",
    "\n",
    "Based on [cs231](http://cs231n.github.io/) by Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall the problem\n",
    "\n",
    "### Moon data with Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "X, y = sklearn.datasets.make_moons(100, shuffle=False, noise=0.35, random_state=42)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = sklearn.datasets.make_moons(100, shuffle=False, noise=0.35, random_state=0)\n",
    "plt.scatter(X_test[:,0], X_test[:,1], s=40, c=y_test, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with Cross-entropy loss and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "Softmax function takes an N-dimensional vector of real numbers and transforms it into a vector of real number in range (0,1) which add upto 1\n",
    "\n",
    "<img src=\"https://eli.thegreenplace.net/images/2016/softmax-layer-generic.png\" alt=\"Softmax\" style=\"width: 50%\"/>\n",
    "\n",
    "\n",
    "\n",
    "$$p_k = \\dfrac{e^{f_k}}{\\sum_{j} e^{f_j}}$$\n",
    "\n",
    "\n",
    "\n",
    "### Cross-entropy loss\n",
    "\n",
    "Cross entropy indicates the distance between what the model believes the output distribution should be, and what the original distribution really is. It is defined as:\n",
    "\n",
    "$$H(y,p) = - \\sum_i y_i log(p_i)$$\n",
    "\n",
    "\n",
    "\n",
    "*Negative Log-Likelihood* (NLL): \n",
    "\n",
    "<img src=\"https://ljvmiranda921.github.io/assets/png/cs231n-ann/neg_log.png\" alt=\"Softmax\" style=\"width: 40%\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$L(\\mathbf{y}) = -\\log(\\mathbf{y})$$\n",
    "\n",
    "\n",
    "<img src=\"https://ljvmiranda921.github.io/assets/png/cs231n-ann/neg_log_demo.png\" alt=\"Softmax\" style=\"width: 90%\"/>\n",
    "\n",
    "Let's take its **derivative**:\n",
    "\n",
    "$$% <![CDATA[\n",
    "\\dfrac{\\partial L_i}{\\partial f_k} = \\dfrac{\\partial L_i}{\\partial p_k} \\dfrac{\\partial p_k}{\\partial f_k}$$\n",
    "$$\\dfrac{\\partial L_i}{\\partial p_k} = -\\dfrac{1}{p_k}$$\n",
    "\n",
    "\n",
    "Derivative of **softmax**:\n",
    "\n",
    "let's $\\Sigma = \\sum_{j} e^{f_j}$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\dfrac{\\partial p_k}{\\partial f_k} &=& \\dfrac{\\partial}{\\partial f_k} \\left(\\dfrac{e^{f_k}}{\\sum_{j} e^{f_j}}\\right) \\\\\n",
    "&=& \\dfrac{\\Sigma \\mathbf{D} e^{f_k} - e^{f_k} \\mathbf{D} \\Sigma}{\\Sigma^2} \\\\\n",
    "&=& \\dfrac{e^{f_k}(\\Sigma - e^{f_k})}{\\Sigma^2}&=& \\dfrac{e^{f_k}}{\\Sigma} \\dfrac{\\Sigma - e^{f_k}}{\\Sigma} \\\\\n",
    "&=& p_k * (1-p_k)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "By combining the two derivatives, we have:\n",
    "\n",
    "$$% <![CDATA[\n",
    "\\begin{eqnarray}\n",
    "\\dfrac{\\partial L_i}{\\partial f_k} &=& \\dfrac{\\partial L_i}{\\partial p_k} \\dfrac{\\partial p_k}{\\partial f_k} \\\\\n",
    "&=& -\\dfrac{1}{p_k} (p_k * (1-p_k)) \\\\\n",
    "&=& (p_k - 1)\n",
    "\\end{eqnarray} %]]>$$\n",
    "\n",
    "Total loss is then defined as the average NLL over the training examples and the regularization:\n",
    "\n",
    "$$L=\\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\frac{1}{2} \\lambda \\sum_k\\sum_l W_{k,l}^2 }_\\text{regularization loss}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://mlfromscratch.com/content/images/2019/12/nn-complete.png\" alt=\"NN1\" style=\"width: 70%;\"/> </td>\n",
    "    <td> <img src=\"https://mlfromscratch.com/content/images/2019/12/dependency-graph2.png\" alt=\"NN2\" style=\"width: 100%;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0] # number of points per class\n",
    "D = X.shape[1] # input dimensionality\n",
    "K = y.max() + 1 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x, deriv=False):\n",
    "    ## Sigmoid\n",
    "    if deriv:\n",
    "        return activation(x) * (1 - activation(x))\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "    ## ReLU   \n",
    "#     if deriv:\n",
    "#         relu_mask = np.array(x, copy = True)\n",
    "#         relu_mask[relu_mask<=0] = 0\n",
    "#         relu_mask[relu_mask>0] = 1\n",
    "#         return relu_mask\n",
    "#     return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, pred_func):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(W, b, x):\n",
    "    hidden_layer = activation(np.dot(x, W[0]) + b[0])\n",
    "    scores = np.dot(hidden_layer, W[1]) + b[1]\n",
    "    return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 20 # size of hidden layer\n",
    "W = 1 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 1 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 0.5\n",
    "reg = 0.0 # regularization strength\n",
    "dropout = 0.0\n",
    "p_keep = 1 - dropout\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(5000):\n",
    "\n",
    "    # evaluate class scores, [N x K]\n",
    "    z = np.dot(X, W) + b\n",
    "    hidden_layer = activation(z) # get output of hidden layer\n",
    "    if dropout > 0:\n",
    "        U1 = (np.random.rand(*hidden_layer.shape) < p_keep) / p_keep # first dropout mask. Notice /p!\n",
    "        hidden_layer *= U1 # drop!\n",
    "    \n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    # Softmax (compute the class probabilities)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss\n",
    "    if i % 1000 == 0:\n",
    "        print(\"iteration %d: loss %f | train accuracy %f  | test accuracy %f | dead neurons %f \" % (\n",
    "                i,\n",
    "                loss,\n",
    "                (np.mean(predict((W, W2), (b, b2), X) == y)),\n",
    "                (np.mean(predict((W, W2), (b, b2), X_test) == y_test)),\n",
    "                sum(hidden_layer.mean(axis=0) < 0.000001)\n",
    "             ))\n",
    "        \n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs.copy()\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden = dhidden * activation(z, deriv=True)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        dhidden *= U1 # drop!\n",
    "        dhidden = dhidden / p_keep\n",
    "    \n",
    "    # finally into W,b\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, y, lambda x: predict((W, W2), (b, b2), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2384/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\" alt=\"Activations\" style=\"width: 80%\"/>\n",
    "\n",
    "\n",
    "Check more functions here - https://mlfromscratch.com/activation-functions-explained/#/\n",
    "\n",
    "*TLDR*: “What neuron type should I use?” Use the **ReLU** non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, try **Leaky ReLU**/**ELU**/**SELU**. Never use sigmoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational power\n",
    "\n",
    "It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see [Approximation by Superpositions of Sigmoidal Function from 1989 (pdf)](https://link.springer.com/article/10.1007/BF02551274), or this [intuitive explanation from Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap4.html)) that given any continuous function $f(x)$ and some $ϵ>0$, there exists a Neural Network $g(x)$ with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $ ∀x,∣f(x)−g(x)∣<ϵ $. In other words, the neural network can approximate any continuous function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "* Mean subtraction\n",
    "* Normalization\n",
    "* PCA and Whitening\n",
    "\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn2/prepro1.jpeg\" alt=\"Normalization\" style=\"width: 80%\"/>\n",
    "<img src=\"http://cs231n.github.io/assets/nn2/prepro2.jpeg\" alt=\"PCA and Whitening\" style=\"width: 80%\"/>\n",
    "\n",
    "\n",
    "### Regularizaion\n",
    "\n",
    "* L1/L2 regularization\n",
    "* Dropout\n",
    "* Augmentations\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn2/dropout.jpeg\" alt=\"Dropout\" style=\"width: 50%\"/>\n",
    "\n",
    "\n",
    "### Parameter Initialization\n",
    "\n",
    "1. ~~All zeros~~\n",
    "1. ~~Constant~~\n",
    "1. Small random numbers. \n",
    "    * `scaling_coef * np.random.randn()` - Normal Distibution\n",
    "    * `scaling_coef * (2* np.random.random() - 1)` - Uniformal Distibution\n",
    "1. Calibrating the variances with 1/sqrt(n) for more details\n",
    "    * `w = np.random.randn(n) / sqrt(n)` where n is the number of its inputs\n",
    "\n",
    "    \n",
    "**Initializing the biases.** It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. For ReLU non-linearities, some people like to use small constant value such as 0.01 for all biases because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement (in fact some results seem to indicate that this performs worse) and it is more common to simply use 0 bias initialization.\n",
    "\n",
    "Check [this article](https://www.deeplearning.ai/ai-notes/initialization/) for more details\n",
    "\n",
    "\n",
    "[**Batch Normalization Layer**](https://arxiv.org/abs/1502.03167)\n",
    "Batch Normalization is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance \n",
    "\n",
    "Check [Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n",
    "\n",
    "### Parameter updates\n",
    "\n",
    "* **Vanilla SGD**\n",
    "\n",
    "```python\n",
    "# Vanilla Minibatch Gradient Descent\n",
    "data_batch = sample_training_data(data, 256) # sample 256 examples\n",
    "weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "weights += - learning_rate * weights_grad # perform parameter update</pre>\n",
    "```\n",
    "\n",
    "* Introduce **Momentum**: \n",
    "\n",
    "```python\n",
    "# Momentum update\n",
    "v = mu * v - learning_rate * weights_grad # integrate velocity\n",
    "weights += v # integrate position\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/nesterov.jpeg\" alt=\"Optimization1\" style=\"width: 80%;\"/>\n",
    "\n",
    "\n",
    "* **Nesterov’s Accelerated Momentum (NAG)** - SGD + Nesterov's Momentum\n",
    "\n",
    "```python\n",
    "weights_ahead = weights + mu * v\n",
    "# evaluate dx_ahead (the gradient at weights_ahead instead of at weights)\n",
    "v = mu * v - learning_rate * weights_ahead_grad\n",
    "weights += v\n",
    "```\n",
    "\n",
    "Per-parameter adaptive learning rate methods:\n",
    "\n",
    "* **Adagrad** - weights that receive high gradients will have their effective learning rate reduced, while weights that receive small or infrequent updates will have their effective learning rate increased. A downside of Adagrad is that in case of Deep Learning, the monotonic learning rate usually proves too aggressive and stops learning too early.\n",
    "\n",
    "```python\n",
    "cache += weights_grad**2\n",
    "weights += - learning_rate * weights_grad / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "* **RMSprop** - RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate \n",
    "\n",
    "\n",
    "```python\n",
    "cache = decay_rate * cache + (1 - decay_rate) * weights_grad**2\n",
    "weights += - learning_rate * weights_grad / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "* **ADAM** - **the default algorithm** to use, and often works slightly better than RMSProp. It looks a bit like RMSProp with momentum. However, it is often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors *m*, *v* are both initialized and therefore biased at zero, before they fully “warm up”\n",
    "\n",
    "```python\n",
    "m = beta1*m + (1-beta1)*weights_grad\n",
    "v = beta2*v + (1-beta2)*(weights_grad**2)\n",
    "weights += - learning_rate * m / (np.sqrt(v) + eps)\n",
    "```\n",
    "    \n",
    "Recommended values in the paper are `eps = 1e-8`, `beta1 = 0.9`, `beta2 = 0.999`.\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/opt2.gif\" alt=\"Optimization1\" style=\"width: 50%;\"/>\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/opt1.gif\" alt=\"Optimization2\" style=\"width: 50%;\"/>\n",
    "\n",
    "Read more about optimizers here - http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "Check more about dynamic learning rate here -  https://www.jeremyjordan.me/nn-learning-rate/\n",
    "\n",
    "more [visualizations](https://dvgodoy.github.io/dl-visuals/Optimizers%20and%20Schedulers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called [torch.autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html). It supports automatic computation of gradient for any computational graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" alt=\"Comp Graph\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Micrograd](https://github.com/karpathy/micrograd) - tiny Autograd engine by Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: MNIST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
    "    plt.title('Class: '+str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*HWhBextdDSkxYvz0kEMTVg.png\" alt=\"Comp Graph\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # map [batch_size, 1, 28, 28] -> [batch_size, 784]\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        train_loss += loss.data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "            \n",
    "    train_loss /= len(train_loader)\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(train_loader.dataset)\n",
    "    \n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    \n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "val_lossv, val_accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, accuracy = train(epoch)\n",
    "    lossv.append(train_loss)\n",
    "    accv.append(accuracy)    \n",
    "    \n",
    "    val_loss, val_accuracy = validate()\n",
    "    val_lossv.append(val_loss)\n",
    "    val_accv.append(val_accuracy)\n",
    "    \n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: ({:.0f}%)\\n'.format(\n",
    "        train_loss, accuracy))\n",
    "    print('Validation set: Average loss: {:.4f}, Accuracy: ({:.0f}%)\\n'.format(\n",
    "        val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), lossv, label='train')\n",
    "plt.plot(np.arange(1,epochs+1), val_lossv, label='val')\n",
    "plt.title('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1,epochs+1), accv, label='train')\n",
    "plt.plot(np.arange(1,epochs+1), val_accv, label='val')\n",
    "plt.title('accuracy');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images, labels = dataiter.next()\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray_r')\n",
    "    ax.set_title(\"{} (gt - {})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'mnist.pt') # you can visualize your model in https://netron.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  -- Homework --\n",
    "\n",
    "1. Download train/test data from https://www.kaggle.com/c/digit-recognizer\n",
    "2. Create validation\n",
    "3. Log training history in order to visualize different experiments on the same graph\n",
    "4. Try to improve pytorch model accurracy and training time:\n",
    "    * introduce learning rate scheduler - https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "    * try other optimizers\n",
    "    * try other activation function\n",
    "    * experiment with model architecture (but use only linear, dropout and batchnorm layers)\n",
    "    \n",
    "5. Submit your best result here https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-course",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
