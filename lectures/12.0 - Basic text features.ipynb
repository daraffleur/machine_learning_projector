{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of Textual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning domain, for example, to perform classification tasks, usually our goal is to find a parametrized model, best in its class: <br><br> $A(X, \\hat{w}): A(X, \\hat{w}) \\simeq f(X) \\Leftrightarrow A(X, \\hat{w}) = \\operatorname*{arg\\,min}_w \\left\\|A(X, w) - f(X)\\right\\|$\n",
    "\n",
    "Where $X \\in R^{ n\\times m}$ - feature matrix ($n$ observations with $m$ features), $w \\in R^{m}$ - vector of model parameters, $\\hat{w}$ - \"best\" model parameters\n",
    "\n",
    "However, as a candidate for X - all that we have <strong>is raw text input, algorithms can't use it as is</strong>\n",
    "\n",
    "In order to apply machine learning on textual data, we first need to transform such content into some numerical format (to form feature vectors). \n",
    "\n",
    "In Natural Language Processing automated feature extraction may be achieved in many ways <strong>(bag-of-words, word embeddings, graph-based representations etc.)</strong>\n",
    "\n",
    "Today, we will dive into details of <strong>bag-of-words</strong> approach and methods, built atop of it in Scikit-Learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intuition Behind the Model. Word Counters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bag-of-words approach we work under the following assumptions:\n",
    "* The text can be analyzed without taking into account the word/token order \n",
    "* We only need to know what words/tokens the text consists of and how many times we met them \n",
    "* The more often a word/token appears in a document, the more important it is \n",
    "\n",
    "More formal, given the collection of texts $T_1, T_2, ... , T_n$, we extract unique tokens $w_1, w_2, ..., w_m$ to form a dictionary.\n",
    "\n",
    "Thus, each text $T_i$ is represented by feature vector $F_j = \\{x_{ij},\\ j \\in [1,m]\\}$, where $x_{ij}$ corresponds to number of occurences of word $w_j$ in text $T_i$\n",
    "\n",
    "Say, out corpus only consists of **2 texts**:\n",
    "\n",
    "[\"I love data science\", \n",
    "\"A data scientist is often smarter than a data analyst\"]\n",
    "\n",
    "\\* **As a preprocessing step, all letters are usually made lowercase, sometimes stemming/lemmatization is performed, as well as stop-words/punctuation removals, but that's not obligatory.**\n",
    "\n",
    "Suppose our tokens are simple unigrams (words), therefore there are **11 unique words**: {i, love, data, science, a, scientist, is, often, smarter, than, analyst}\n",
    "\n",
    "Then, our corpus is mapped to feature vectors $T_1=(1,1,1,1,0,0,0,0,0,0,0)$, $T_2=(0,0,2,0,2,1,1,1,1,1,1)$\n",
    "\n",
    "|Text #|i|love|data|science|a|scientist|is  |often|smarter|than|analyst|\n",
    "|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "|$T_1$|1|1|1|1|0|0|0|0|0|0|0|\n",
    "|$T_2$|0|0|2|0|2|1|1|1|1|1|1|\n",
    "\n",
    "Well, how memory-effective this approach is?\n",
    "If n == 20k, this textual corpus might spawn a dictionary with around 100k elements. \n",
    "<br>Thus, storing X as an array of type int32 would require 20000 x 100000 x 4 bytes ~ **8GB in RAM** which is barely manageable on today’s computers.\n",
    "\n",
    "Fortunately, **most values in X will be zeros** since for a given document less than a couple thousands (or even hundreds) of distinct words will be used. For this reason we say that bags of words are **typically high-dimensional sparse datasets**. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "Sparse matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.\n",
    "\n",
    "#### Pros\n",
    "* Very intuitive approach, easy to use, understand and apply - you can code it yourself\n",
    "* Built-in support in many scientific/NLP libraries\n",
    "* Memory-efficient sparse format, acceptable by most algorithms \n",
    "* Despite its simplicity, works well, good results could be reached\n",
    "* Fast preprocessing, even on 1 core\n",
    "\n",
    "#### Cons\n",
    "* Huge corpus usually leads to huge vocabulary size (millions of words), even sparse format wouldn't help you (only hashing tricks)\n",
    "* There are other approaches, manageable to catch more details (semantics, relations, structure) - word embeddings etc.\n",
    "* A bag of words is an orderless representation: throwing out spatial relationships between features leads to the fact that simplified model cannot let us to distinguish between sentences, built from the same words while having opposite meanings:\n",
    "    * \"New episodes **don't** feel like the first - watch it!\" (positive)\n",
    "    * \"New episodes feel like the first - **don't** watch it!\" (negative)\n",
    "* **However, it is somehow treated by increasing the \"length\" of the token (unigrams $\\rightarrow$ bigrams, n-grams etc.), gluing negative particles with next word (not like $\\rightarrow$ not_like), using character n-grams, skip-grams etc.** (see [this section for n-grams details](#3_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Capturing Dependencies. N-grams Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Bag-of-Words(BoW) model, built on simple tokens (unigrams), is too simplified and catch no spatial dependencies.\n",
    "To deal with it and to expand our knowledge, let's briefly recall what a **N-gram** is:\n",
    "* N-gram is a sequence of $N$ basic tokens. \n",
    "* N-grams can be defined in different ways, based on token definition. ('word', 'character', 'character_wb' etc.)\n",
    "\n",
    "1) **Word n-grams: (to catch more semantics)** \n",
    "* unigrams: 'I love data science' $\\rightarrow$ [i, love, data, science]\n",
    "* bigrams (2-grams): 'I love data science' $\\rightarrow$ [i love, love data, data science]\n",
    "* 3-grams: 'I love data science' $\\rightarrow$ [i love data, love data science]\n",
    "* ...\n",
    "\n",
    "2) **Character n-grams: (allows to catch features like \":)\", deal somehow with misspeled words like \"looong\" etc.)**\n",
    "* 5-grams: 'I love data science' $\\rightarrow$ [\"i lov\", \" love\", ... , \"cienc\", \"ience\"]\n",
    "* ...\n",
    "\n",
    "3) **Character-wb n-grams (n-grams, only in word boundaries):**\n",
    "* 5-grams: 'I love data science' $\\rightarrow$ {\" i \", \" love\", \"love \", ... , \"cienc\", \"ience\"]\n",
    "* ...\n",
    "\n",
    "4) **Skip-n-grams or k-skip-n-grams (both character- and word-based, extends spatial dependencies)**\n",
    "* A sequence of $N$ basic tokens, having distance of $\\leq K$ tokens between them\n",
    "* 1-skip-2-grams: 'I love data science' $\\rightarrow$ [i data, love science]\n",
    "* ...\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "#### PROS\n",
    "\n",
    "The same as in Bag-of-Words + more context can be captured\n",
    "\n",
    "#### CONS\n",
    "\n",
    "Don't forget that with the increase of n-gram range the vocabulary **rapidly grows up**!\n",
    "<br>**|(1,1)-grams| << |(1,2)-grams| << |(1,3)-grams| << ...**\n",
    "<br>where (1,1)-grams = unigrams, (1,2)-grams = unigrams AND bigrams, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  CountVectorizer\n",
    "\n",
    "CountVectorizer in Sklearn implements aforementioned Bag-of-Words approach:\n",
    "\n",
    "**Commonly used parameters:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - what token to use (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - what N to use: say, ngram_range=(1,2) $\\rightarrow$  use both unigrams and bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} - whether to filter stop-words or not\n",
    "* **vocabulary**={None, your_own_dictionary} - whether to use given vocabulary or to build it from extracted tokens\n",
    "* **max_features**={N, None} - to build a vocabulary that consider **top-N** terms ordered by term frequency (TF) across the corpus\n",
    "* **max_df** – when building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
    "* **min_df** – when building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage example\n",
    "\n",
    "# import CountVectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer object\n",
    "cv = CountVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=1.0, # don't filter words by their frequency\n",
    "                    max_features=6 # only top-6 words will be used as columns\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using it as an example for the other feature extraction methods\n",
    "# You can use iterables, numpy arrays, pandas DataFrames as an input.\n",
    "texts = [\n",
    "    'nobody can stop me', # \"stop\" will be filtered by stop_words list\n",
    "    'word is a building blocks of a text', # \"word\" will be filtered by stop_words list\n",
    "    'I like doing feature extraction on text',\n",
    "    'I do not like digits in text like 12345'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply CountVectorizer to text corpus\n",
    "transformed_texts_cv = cv.fit_transform(texts)\n",
    "# convert sparse representation of transformed texts to dense format and explore it\n",
    "print('Obtained feature matrix X:')\n",
    "print(transformed_texts_cv.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dictionary (sorted by column index) to see mapping between indices/columns and words \n",
    "print('Dictionary:')\n",
    "for k,v in sorted(cv.vocabulary_.items(), reverse=False):\n",
    "    print('column index:{}, token: {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform new sentences (having CountVectorizer trained)\n",
    "new_text = ['i like feature extraction very much'] \n",
    "new_transformed = cv.transform(new_text)\n",
    "# some words, like \"very\" and \"much\", were not used to build the dictionary, thus, they will be skipped\n",
    "print('\\nNew sentence (transformed):')\n",
    "print(new_transformed.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TF-IDF Augmentation. TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TF-IDF approach (term frequency - inverse document frequency), in addition to usual BoW-model, the following augmentation is made:\n",
    "* The text can be analyzed without taking into account the word/token order\n",
    "* We only need to know what words/tokens the text consists of and how many times we met them\n",
    "* The more often a word/token appears in a document, the more important it is\n",
    "* **If a word/token appears in a document, but rarely appears in other documents - it is important and vice versa: <br>if its commonly across most documents - then we cannot rely on this word to help us distinquish between texts** \n",
    "\n",
    "Thus, we are looking on the whole corpus, usual word counters (term frequencies, TF) are weighted by IDF multiplier:\n",
    "\n",
    "$$  \n",
    "    \\begin{cases} TF(w,T)=n_{Tw} \\\\ IDF(w, T)= log{\\frac{N}{n_{w}}}\\end{cases} \\implies \n",
    "    TF\\text{-}IDF(w, T) = n_{Tw}\\ log{\\frac{N}{n_{w}}} \\ \\ \\ \\ \\forall w \\in W\n",
    "$$\n",
    "\n",
    "<br> where $T$ corresponds to current document (text), \n",
    "<br>$w$ - selected word in document T, \n",
    "<br>$n_{Tw}$ - number of occurences of $w$ in text $T$, \n",
    "<br>$n_{w}$ - number of documents, containing word $w$, \n",
    "<br> $N$ - total number of documents in a corpus.\n",
    "\n",
    "\n",
    "**Commonly used parameters:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - what token to use (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - what N to use: say, ngram_range=(1,2) $\\rightarrow$  use both unigrams and bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} (default) - whether to filter stop-words or not\n",
    "* **vocabulary**={None, your_own_dictionary} - whether to use given vocabulary or to build it from exracted tokens\n",
    "* **max_features**={N, None} - to build a vocabulary that only consider the top N ordered by term frequency across the corpus\n",
    "* **norm**={‘l1’, ‘l2’ or None, optional} - norm feature vector to unit norm ($L_2-$, $L_1-$ norms)\n",
    "* **smooth_idf**={True, False} Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "* **max_df** – when building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
    "* **min_df** – when building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage example\n",
    "\n",
    "# import TfidfVectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create TfidfVectorizer object\n",
    "tv = TfidfVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    vocabulary=None, # or vocabulary=your_own_dictionary\n",
    "                    max_df=1.0, # don't filter words by their frequency\n",
    "                    max_features=6, # only top-6 words will be used as columns,\n",
    "                    smooth_idf=True,\n",
    "                    norm='l2' # euclidean norm is used by default\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply TfidfVectorizer to the same text corpus\n",
    "transformed_texts_tv = tv.fit_transform(texts)\n",
    "# convert sparse representation of transformed texts to dense format and explore it\n",
    "print('Obtained feature matrix X (see, L2-norm is used):')\n",
    "print(transformed_texts_tv.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dictionary (sorted by column index) to see mapping between indices/columns and words \n",
    "print('Dictionary:')\n",
    "for k,v in sorted(tv.vocabulary_.items(), reverse=False):\n",
    "    print('column index:{}, token: {}'.format(v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform new sentences (having TfidfVectorizer trained)\n",
    "new_text = ['i like extraction very much'] \n",
    "new_transformed = tv.transform(new_text)\n",
    "# \"very\", \"much\" etc. were not used to build the dictionary, thus, they will be skipped\n",
    "print('\\nNew sentence (transformed):')\n",
    "print(new_transformed.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Hashes. HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hash function is any function that **can be used to map data of arbitrary size to data of fixed size**. \n",
    "<br>The values returned by a hash function are called hash values, hash codes, or simply hashes.\n",
    "<br>$f(X) \\rightarrow \\{0,N-1\\}:\\ f(X) = X\\  mod\\ N$, function, that maps input into a set of $N$ \"buckets\", is an example of a hash function:\n",
    "\n",
    "Say, $N = 2^k = 2^3 = 8$, then $\\ f(15)=15\\ mod \\ 8 = 7,\\ f(9)=9\\ mod \\ 8 = 1,\\ ...$\n",
    "\n",
    "This vectorizer implementation uses the hashing trick to find the mapping of **token string name** to **feature integer index**.\n",
    "\n",
    "#### PROS:\n",
    "\n",
    "* **Very memory-scalable to large datasets** as there is no need to store a vocabulary dictionary in memory\n",
    "* Fast to serialize/deserialize as it holds no state besides the constructor parameters\n",
    "* Can be used in a streaming (partial fit) and/or be parallelized as there is no state computed during fit\n",
    "* Can be used as a \"silly\" dimensionality reduction\n",
    "\n",
    "#### CONS (vs Vectorizers with in-memory vocabulary): \n",
    "\n",
    "* There is no way to compute the inverse transform (to get from feature indices to string feature names) <br> which **can be a problem when trying to introspect which features are most important to a model**.\n",
    "* There can be **collisions**: distinct tokens can be mapped to the same \"bucket\" (feature index). \n",
    "<br>However, in practice this is rarely an issue if number of bins is large enough (e.g. $2^{18}$ for text classification problems)\n",
    "\n",
    "\n",
    "\\* The hash function used is the signed 32-bit version of Murmurhash3 (for those, who are really interested :)  )\n",
    "\n",
    "**Commonly used parameters:**\n",
    "* **analyzer**={‘word’, ‘char’, ‘char_wb’} - what token to use (word, char-n-grams etc.)\n",
    "* **ngram_range**=(min_n, max_n) - what N to use: say, ngram_range=(1,2) $\\rightarrow$  use both unigrams and bigrams\n",
    "* **stop_words**={‘english’, list_of_words, or None} (default) - whether to filter stop-words or not\n",
    "* **n_features**={N} - how many \"buckets\" to use\n",
    "* **norm**={‘l1’, ‘l2’ or None, optional} - norm feature vector to unit norm ($L_2-$, $L_1-$ norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage example\n",
    "\n",
    "# import HashingVectorizer from sklearn library\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# create HashingVectorizer object\n",
    "hv = HashingVectorizer(\n",
    "                    analyzer='word', # token = word\n",
    "                    ngram_range=(1,1), # only unigrams are used, (1,2) - unigrams/bigrams, ..., etc.\n",
    "                    stop_words=['my', 'stop', 'word', 'list'], # or stop_words='english'\n",
    "                    n_features=6, # only 6 bins will be used as columns, high probability of collisions!\n",
    "                    norm=None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply HashingVectorizer to the same text corpus\n",
    "transformed_texts_hv = hv.fit_transform(texts)\n",
    "# convert sparse representation of transformed texts to dense format and explore it\n",
    "print('Obtained feature matrix X (see, no norm is used):')\n",
    "print(transformed_texts_hv.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I see no dictionary ...\n",
    "print('Dictionary:')\n",
    "print('Oops, Hashing trick assumes no vocabulary will be used at all, online learning :)')\n",
    "print(\"However, we won't be able to do reverse transform and to get exact words :( \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# transform new sentences (having HashingVectorizer trained)\n",
    "new_text = ['i like extraction very much'] \n",
    "new_transformed = hv.transform(new_text)\n",
    "# \"very\", \"much\" etc. were not used to build the dictionary, thus, they will be skipped\n",
    "print('\\nNew sentence (transformed):')\n",
    "print(new_transformed.todense(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Going Beyond: Feature Engineering\n",
    "\n",
    "Usually, specific domain leads to specific information, hidden inside of your data. \n",
    "You need to extract it, as much as possible. \n",
    "\n",
    "For example, if we want to run sentiment analysis (classification task) on the IMDB dataset (movie reviews) and it seems to us that **many reviews may contain explicit marks (say, in a form of x/xx)**, than we should check this out and extract useful custom feature:\n",
    "\n",
    "[\"Average film, however, starring Matt Damon, 8/10\", 1] $\\rightarrow$ {\"8/10\"} $\\rightarrow$ 8/10=0.8 ~ 1 $\\rightarrow$ review is positive\n",
    "<br>[\"2/10, there is nothing to add\", 0] $\\rightarrow$ {\"2/10\"} $\\rightarrow$ 2/10=0.2 ~ 0 $\\rightarrow$ review is negative.\n",
    "\n",
    "However, be aware of dates and outliers (in relation to this particular feature) or whatever else - always check your code / regular expressions:\n",
    "\n",
    "Say, incorrect parsing of **'01/10/1999'** would lead to **{1/10, 10/1999} or {1/10}  ~ 0 (negative review?!)** errors.\n",
    "\n",
    "### Hereinafter, we'll discuss domain specific features, they are no panacea in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Token-based Level\n",
    "\n",
    "We need to look on tokens (words, entities like smiles etc.) and try to extract meaningful features\n",
    "\n",
    "* positive smiles\n",
    "* negative smiles\n",
    "* explicit rating (marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import Word, TextBlob\n",
    "import re # for regular expressions\n",
    "\n",
    "# download resources to be used by TextBlob wrapper (if not yet downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implemenation does not deal with aforementioned cases, \n",
    "# to extract rating \"candidates\" in a text s\n",
    "def get_rate(s):\n",
    "    # searching for possible candidates\n",
    "    candidates = re.findall(r'(\\d{1,3}[\\\\|/]{1}\\d{1,2})', s)\n",
    "    rates = []\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            rates.append(eval(c)) # by the way, \"eval\" is a prime evil, it may lead you to the dark side :)\n",
    "            # instead, say, install sympy\n",
    "            # from sympy import sympify\n",
    "            # sympify(\"1*5/6*(7+8)\").evalf()\n",
    "        except SyntaxError:\n",
    "            pass\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "    return np.mean(rates) if rates else -1 # if there is more than 1 value, calculate mean\n",
    "\n",
    "# bags of positive/negative smiles\n",
    "positive_smiles = set([\n",
    "\":‑)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‑D\",\":D\",\"8‑D\",\"8D\",\n",
    "\"x‑D\",\"xD\",\"X‑D\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‑)\",\";)\",\"*-)\",\"*)\",\";‑]\",\";]\",\";^)\",\":‑,\",\";D\",\":‑P\",\":P\",\"X‑P\",\"XP\",\n",
    "\"x‑p\",\"xp\",\":‑p\",\":p\",\":‑Þ\",\":Þ\",\":‑þ\",\":þ\",\":‑b\",\":b\",\"d:\",\"=p\",\">:P\", \":'‑)\", \":')\",  \":-*\", \":*\", \":×\"\n",
    "])\n",
    "negative_smiles = set([\n",
    "\":‑(\",\":(\",\":‑c\",\":c\",\":‑<\",\":<\",\":‑[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‑':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‑/\",\n",
    "\":/\",\":‑.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‑|\",\":|\",\"|‑O\",\"<:‑|\"\n",
    "])\n",
    "\n",
    "# function to extract token-level features from texts\n",
    "def get_token_level_features(texts, visualize=True):\n",
    "    \n",
    "    # assume texts = pd.Series with review text\n",
    "    print('extracting token-level features...')\n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['text'] = texts # this is our review\n",
    "    \n",
    "    # 1. extract rating, like \"great film. 9/10\" will yield 0.9\n",
    "    tdf['rating'] = tdf['text'].apply(get_rate).fillna(-1) # rating (if found in review, else substitute NaN's by -1)\n",
    "\n",
    "    # 2. extract smiles and count positive/negative smiles per review\n",
    "    tdf['positive_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in positive_smiles]))\n",
    "    tdf['negative_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in negative_smiles]))\n",
    "    \n",
    "    if visualize:\n",
    "        # this is used for visual clarity, return pd.DataFrame\n",
    "        return tdf \n",
    "    else:\n",
    "        # get correct (and sparse) representation of feature matrix F\n",
    "        from scipy.sparse import csr_matrix \n",
    "        return csr_matrix(tdf[tdf.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentence-based / Text-based Level\n",
    "\n",
    "We moved up to sentence/text level.\n",
    "<br><i>Someone can argue about level of these features, but let us just put them here</i>\n",
    "<br>Let's see what features we can search for:\n",
    "* **Sentence count** (text must be split into sentences, then extract length of obtained list) \n",
    "* **Exclamation marks count** (integer) or presence (boolean) - catching stress, expecially if we use probabilistic output instead of binary classification\n",
    "* **Question marks count** (integer) or presence (boolean) - can sometimes help in catching sarcasm\n",
    "* **Uppercase word count** (of length > 1, to omit \"A\"s) - stress of a text, expecially if we use probabilistic output instead of binary classification\n",
    "* **Contrast conjugations**, like {'instead','nevertheless','on the contrary','on the other hand'} - to catch possible changes of a sentiment\n",
    "\n",
    "Some information regarding text \"edges\" - first/last sentences in a review:\n",
    "* **\"polarity\" of first/last sentence[s]**\n",
    "* **\"subjectivity\" of first/last sentence[s]**\n",
    "* **\"purity\" of first/last sentence[s] or the whole set of sentences** - to catch a change of a sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's continue...\n",
    "\n",
    "# contrast conjugations\n",
    "contrast_conj = set([\n",
    "'alternatively','anyway','but','by contrast','differ from','elsewhere','even so','however','in contrast','in fact',\n",
    "'in other respects','in spite of','in that respect','instead','nevertheless','on the contrary','on the other hand',\n",
    "'rather','though','whereas','yet'])\n",
    "\n",
    "# to get review \"purity\" ~ shows same sentiment over review (~1) or changing sentiment (~0)\n",
    "def purity(sentences):\n",
    "    # obtain polarities across the sentences\n",
    "    polarities = np.array([TextBlob(x).sentiment.polarity for x in sentences])\n",
    "    return polarities.sum() / np.abs(polarities).sum()\n",
    "\n",
    "# uppercase pattern\n",
    "uppercase_pattern = re.compile(r'(\\b[0-9]*[A-Z]+[0-9]*[A-Z]+[0-9]*\\b)')\n",
    "\n",
    "# regular expression to split review on sentences, you can use inline textblob object field: TextBlob(x).sentences_\n",
    "sentence_splitter = re.compile('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\!|\\?|\\.)\\s')\n",
    "# you can https://regex101.com/ for regex creation/checking, very convenient\n",
    "\n",
    "# feature engineering\n",
    "def get_text_level_features(texts, visualize=True):\n",
    "    # assume text = pd.Series with review text\n",
    "    print('extracting text-level features...')\n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['text'] = texts # this is our review\n",
    "    tdf['sentences'] = tdf.text.apply(lambda s: re.split(sentence_splitter, s)) # split it into sentences\n",
    "    \n",
    "    tdf['sentence_cnt'] = tdf['sentences'].apply(len) # sentence count\n",
    "    tdf['exclamation_cnt'] = tdf.text.str.count('\\!') # exclamation mark count\n",
    "    tdf['question_cnt'] = tdf.text.str.count('\\?') # question mark count\n",
    "    \n",
    "    # uppercase words cnt (like HOLY JESUS!)\n",
    "    tdf['upper_word_cnt'] = tdf.text.apply(lambda s: len(re.findall(uppercase_pattern, s)))\n",
    "    \n",
    "    # not so informative, but still - contrast conjugations\n",
    "    tdf['contrast_conj_cnt'] = tdf.text.apply(lambda s: len([c for c in contrast_conj if c in s]))\n",
    "    \n",
    "    # polarity of 1st sentence\n",
    "    tdf['polarity_1st_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[0]).sentiment.polarity)\n",
    "    # subjectivity of 1st sentence\n",
    "    tdf['subjectivity_1st_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[0]).sentiment.subjectivity)\n",
    "    # polarity of last sentence\n",
    "    tdf['polarity_last_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[-1]).sentiment.polarity)\n",
    "    # subjectivity of last sentence\n",
    "    tdf['subjectivity_last_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[-1]).sentiment.subjectivity)\n",
    "    # subjectivity of review itself\n",
    "    tdf['polarity'] = tdf.text.apply(lambda s: TextBlob(s[-1]).sentiment.polarity)\n",
    "    # \"purity\" of review, |sum(sentence polarity) / sum(|sentence polarity|))|, ~ 1 is better, ~ 0 -> mixed\n",
    "    tdf['purity'] = tdf.sentences.apply(purity)\n",
    "    tdf['purity'].fillna(0, inplace=True)\n",
    "    \n",
    "    if visualize:\n",
    "        # this is used for visual clarity, return pd.DataFrame\n",
    "        return tdf \n",
    "    else:\n",
    "        # get correct (and sparse) representation of feature matrix F\n",
    "        from scipy.sparse import csr_matrix \n",
    "        return csr_matrix(tdf[tdf.columns[2:]].values)\n",
    "\n",
    "### BE CAREFUL, if you use LINEAR MODELS and have MOSTLY SHORT REVIEWS (1 sentence), then\n",
    "### tdf['subjectivity_1st_sent'] ~ tdf['subjectivity_last_sent'], two same columns, leads to multicollinearity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test custom features:\n",
    "\n",
    "reviews = [\n",
    "    \"Waste of time :( 2/10 for the plot and 4/10 for acting!\",\n",
    "    'Awful film! Nobody can like it',\n",
    "    'Wow! Am I impressed?? TOTALLY :D',\n",
    "    '7/10'\n",
    "]\n",
    "\n",
    "# token-based\n",
    "token_lf = get_token_level_features(reviews)\n",
    "token_lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# token-based\n",
    "token_lf = get_text_level_features(reviews)\n",
    "token_lf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(clf__C=[10, 1, 0.1, 0.01])\n",
    "grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(C=1)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projector_course",
   "language": "python",
   "name": "projector_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
