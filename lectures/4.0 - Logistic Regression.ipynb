{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логістична регресія\n",
    "\n",
    "[A Visual Introduction To Logistic Regression](https://mlu-explain.github.io/logistic-regression/)\n",
    "\n",
    "Розглянемо задачу класифікації на два класи. Використовуючи [теорему Байєса](https://uk.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D1%94%D1%81%D0%B0) ми можемо записати ймовірність класу при заданих ознаках (для простоти нехай буде лише ознака):\n",
    "\n",
    "$$\\large \\begin{array}{rcl} p\\left(c = 0 \\mid x \\right) &=& \\frac{p\\left(x \\mid c = 0\\right) p\\left(c = 0\\right)} {p\\left(x \\mid c = 0\\right) p\\left(c = 0\\right) + p\\left(x \\mid c = 1\\right) p\\left(c = 1\\right)}  \\\\\n",
    "&=& \\frac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "де:\n",
    "$$\\large a = \\ln \\frac{p\\left(x \\mid c = 0\\right) p\\left(c = 0\\right)}{p\\left(x \\mid c = 1\\right) p\\left(c = 1\\right)}$$\n",
    "$\\sigma\\left(a\\right)$ називається [сигмоїдом](https://uk.wikipedia.org/wiki/%D0%A1%D0%B8%D0%B3%D0%BC%D0%BE%D1%97%D0%B4%D0%B0) або логістичною кривою.\n",
    "\n",
    "Цікавою особливістю сигмоїда і те, що його похідна виражається через значення самого себе:\n",
    "\n",
    "$${\\large  \\sigma(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{1+e^{x}}},}$$\n",
    "\n",
    "\n",
    "$$\\large \\frac{\\partial \\sigma\\left(x\\right)}{\\partial x} = {\\frac {e^{x}\\cdot (1+e^{x})-e^{x}\\cdot e^{x}}{(1+e^{x})^{2}}}={\\frac {e^{x}}{(1+e^{x})^{2}}}=\\sigma(x){\\big (}1-\\sigma(x){\\big )}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graphics in the notebook \n",
    "%matplotlib inline\n",
    "# support operations for large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "# make experiments reproducible\n",
    "np.random.seed(12345)\n",
    "# extension of main plotting library matplotlib\n",
    "import seaborn as sns\n",
    "# main library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# set style\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "# set default size of plots\n",
    "plt.rcParams['figure.figsize'] = 16, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scalar function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# make vectorized function from scalar\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "plt.axvline(0, color='black', linestyle='-', label='origin')\n",
    "plt.axhline(0, color='black', linestyle='-')\n",
    "plt.plot(x, sigmoid(x))\n",
    "# plt.plot(x, dsigmoid(x))\n",
    "plt.title('Sigmoid function', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear_logistic_regression](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1534281070/linear_vs_logistic_regression_edxw03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Припустимо, що ознаки всередині класів розподілені нормально зі своїм середнім та дисперсією:\n",
    "$$\\large p\\left( x \\mid c = k \\right) \\sim \\mathcal{N}\\left( \\mu_k, \\sigma \\right)$$\n",
    "\n",
    "а також, для початку, припустимо, що класи рівноймовірні:\n",
    "$$\\large \\begin{array}{rcl} a &=& \\ln \\frac{p\\left(x \\mid c = 0\\right)}{p\\left(x \\mid c = 1\\right)} \\\\\n",
    "&=& \\ln p\\left(x \\mid c = 0\\right) - \\ln p\\left(x \\mid c = 1\\right)\\\\\n",
    "&=& \\frac{1}{2\\sigma^2} \\left(\\mu_1 - x\\right)^2 -\\frac{1}{2\\sigma^2} \\left(\\mu_0 - x\\right)^2 \\\\\n",
    "&=& \\frac{1}{2\\sigma^2} \\left(\\left(\\mu_1 - x\\right)^2 - \\left(\\mu_0 - x\\right)^2\\right) \\\\\n",
    "&=& \\frac{1}{2\\sigma^2} \\left(\\mu_1^2 - 2\\mu_1 x + x^2 - \\mu_0^2 + 2 \\mu_0 x - x^2\\right) \\\\\n",
    "&=& \\frac{\\mu_1^2 - \\mu_0^2}{2\\sigma^2} + \\frac{\\mu_0 - 2\\mu_1}{\\sigma^2}x \\\\\n",
    "&=& w_0 + w_1 x\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Виходить, що функція логістичного сигмоїду від лінійної комбінації ознак та параметрів природно спливає при припущенні про нормальний розподіл ознак.\n",
    "\n",
    "Відповідно для задачі класифікації ми можемо сформувати гіпотези у вигляді:\n",
    "\n",
    "$$\\large \\forall h \\in \\mathcal{H}, h\\left(\\vec{x}\\right)\n",
    "= \\frac{1}{1 + e^{ -\\sum_{i=0}^m w_i x_i \\\\}}=\\frac{1}{1 + e^{-\\vec{x}^T \\vec{w}}}\n",
    "$$\n",
    "\n",
    "де:\n",
    "* $\\large \\vec{x} \\in \\mathbb{R}^{m + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Згадаймо розподіл Бернуллі - це розподіл випадкової величини, яка набуває значення $1$ з ймовірністю $\\large p$, і значення $0$ з ймовірністю $\\large q = 1 - p$:\n",
    "$$\\large p\\left(k \\mid p\\right) = p^k \\left(1 - p\\right)^{1 - k}$$\n",
    "\n",
    "Короткий приклад: монетку підкинули $ \\large n + m $ раз, $ \\large n $ раз випав орел і $ \\large m $ раз решка. Знайдемо [оцінку максимальної правдоподібності](https://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%97_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D1%96%D0%B1%D0%BD%D0%BE%D1%81%D1%82%D1%96) ймовірність випадіння орла $\\large p(\\text{head}) = p$\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\\mathcal{L} &=& \\ln p^n \\left(1 - p\\right)^m \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p} &=& \\frac{n}{p} - \\frac{m}{1 - p} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p} = 0 &\\Leftrightarrow& \\frac{n}{p} = \\frac{m}{1 - p} \\\\\n",
    "&\\Leftrightarrow& p = \\frac{n}{n + m}\n",
    "\\end{array}$$\n",
    "\n",
    "Зробимо те саме для набору даних $\\large D = \\left\\{\\left(\\vec{x}_i, y_i\\right)\\right\\}_{i=1,\\ldots,n}$, $\\large \\forall i, y_i \\in \\left\\{0, 1\\right\\}$, тоді правдоподібність набору:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\\mathcal{L} &=& \\ln \\prod_{i=1}^n p\\left(c = 0 \\mid \\vec{x}_i \\right)^{y_i} \\left(1 - p\\left(c = 0 \\mid \\vec{x}_i \\right)\\right)^{1 - y_i} \\\\\n",
    "&=& \\ln \\prod_{i=1}^n \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right)^{y_i} \\left(1 - \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right)\\right)^{1 - y_i} \\\\\n",
    "&=& \\sum_{i=1}^n y_i \\ln \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right) + \\left(1 - y_i\\right) \\ln \\left(1 - \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Знайдемо формулу оновлення ваг логістичної регресії для градієнтного спуску:\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}}\\sum_{i=1}^n y_i \\ln \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right) + \\left(1 - y_i\\right) \\ln \\left(1 - \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right)\\right) \\\\\n",
    "&=& \\sum_{i=1}^n y_i \\frac{1}{\\sigma} \\sigma \\left(1 - \\sigma\\right) \\vec{x}_i + \\left(1 - y_i\\right) \\frac{1}{1 - \\sigma} \\left(-1\\right)\\sigma \\left(1 - \\sigma\\right) \\vec{x}_i \\\\\n",
    "&=& \\sum_{i=1}^n y_i \\left(1 - \\sigma\\right) \\vec{x}_i - \\left(1 - y_i\\right) \\sigma \\vec{x}_i \\\\\n",
    "&=& \\sum_{i=1}^n \\vec{x}_i \\left(y_i - \\sigma \\left(\\vec{w}^T \\vec{x}_i\\right) \\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Тоді ми можемо знайти оптимальні ваги за допомогою градієнтного спуску:\n",
    "\n",
    "$$\\large \\vec{w}_{\\text{new}} := \\vec{w} + \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}}$$\n",
    "де:\n",
    "* $\\large \\alpha$ - це швидкість навчання (*learning rate*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "plt.rcParams['figure.figsize'] = 12, 10\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"Base class for classifiers.\"\"\"\n",
    "\n",
    "    def fit(X, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(X):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Classifier):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 add_intercept=True,\n",
    "                 learning_rate=0.01,\n",
    "                 max_iteration=1000,\n",
    "                 verbose=0,\n",
    "                 early_stopping=True,\n",
    "                 max_iter_without_loss=5\n",
    "                ):\n",
    "        self.add_intercept = add_intercept\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iteration = max_iteration\n",
    "        self.verbose = verbose\n",
    "        self.early_stopping = early_stopping\n",
    "        self.max_iter_without_loss = max_iter_without_loss\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _log_loss(y_true, y_pred):\n",
    "        return - np.mean(y_true*np.log(y_pred) + (1-y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept, X))\n",
    "            \n",
    "            \n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "    \n",
    "        min_loss = 10**6\n",
    "        n_iter_no_change = 0\n",
    "        for i in range(self.max_iteration):\n",
    "            # get preds\n",
    "            scores = np.dot(X, self.weights)\n",
    "            predictions = self._sigmoid(scores)\n",
    "\n",
    "            # how much did we miss?\n",
    "            loss = self._log_loss(y, predictions)\n",
    "            d_loss = y - predictions\n",
    "            \n",
    "            if self.verbose:\n",
    "                if (i % 1) == 0:\n",
    "                    print(loss)\n",
    "\n",
    "            # update weights\n",
    "            self.weights = self.weights + self.learning_rate * np.dot(X.T, d_loss)\n",
    "            \n",
    "            #early stopping\n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                n_iter_without_loss = 0\n",
    "            else:\n",
    "                n_iter_without_loss += 1\n",
    "                \n",
    "                \n",
    "            if self.early_stopping:\n",
    "                if n_iter_without_loss == self.max_iter_without_loss:\n",
    "                    if self.verbose:\n",
    "                        print(f'Stopped on {i} interation with loss {min_loss}')\n",
    "                    break\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.add_intercept:\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept, X))\n",
    "            \n",
    "        scores = np.dot(X, self.weights)\n",
    "        predictions = self._sigmoid(scores)\n",
    "        return predictions\n",
    "                    \n",
    "            \n",
    "    def predict(self, X):\n",
    "        return np.round(self.predict_proba(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SkLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = SkLogisticRegression()\n",
    "lr = LogisticRegression(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lr.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мультикласова класифікація"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = sklearn.datasets.make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SkLogisticRegression(multi_class='ovr') # multi_class : {'ovr', 'multinomial'}\n",
    "lr.fit(X, y)\n",
    "plot_decision_boundary(lr.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SkLogisticRegression(multi_class='multinomial') # multi_class : {'ovr', 'multinomial'}\n",
    "lr.fit(X, y)\n",
    "plot_decision_boundary(lr.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.HW  - Реалізувати [softmax](https://en.wikipedia.org/wiki/Softmax_function) регресію для мультикласової класифікації***\n",
    "\n",
    "$$ \\sigma ({z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}\\ \\ \\ \\ {\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}  {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класифікація тексту"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спробуємо передбачити \"токсичність\" тексту із змагання Kaggle [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "Дані можна завантажити тут - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_toxic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tf-idf](https://habrastorage.org/files/a0a/bb1/2e9/a0abb12e9ed94624ade0b9090d26ad66.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features = word_vectorizer.fit_transform(train['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1000\n",
    "target = 'toxic'\n",
    "\n",
    "x_train = train_word_features[:-test_size]\n",
    "x_test = train_word_features[-test_size:]\n",
    "y_train = train[target][:-test_size]\n",
    "y_test = train[target][-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = SkLogisticRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналіз моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "    hi,idiot, why are you delate my talking, just come out say \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(word_vectorizer.transform([test_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict_proba(word_vectorizer.transform([test_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.explain_weights(lr, feature_names=word_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eli5.explain_prediction(lr, test_text, vec=word_vectorizer, feature_names=word_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.HW Знайти оптимальий класифікатор та відповісти на питання -  https://forms.gle/NGP1G9FmvzTzq43k9***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Links:\n",
    "* [cs229 by Andrew Ng](https://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf)\n",
    "* [A Visual Introduction To Logistic Regression](https://mlu-explain.github.io/logistic-regression/)\n",
    "\n",
    "based on https://mlcourse.ai/book/topic04/topic4_linear_models_part1_mse_likelihood_bias_variance.html by Pavel Nesterov \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-course",
   "language": "python",
   "name": "ml-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
