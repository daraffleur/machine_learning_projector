{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation\n",
    "\n",
    "If a machine learning model performs well, why do not we just trust the model and ignore why it made a certain decision? “The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks.”\n",
    "\n",
    "Let us dive deeper into the reasons why interpretability is so important. When it comes to predictive modeling, you have to make a trade-off: Do you just want to know what is predicted? For example, the probability that a customer will churn or how effective some drug will be for a patient. Or do you want to know why the prediction was made and possibly pay for the interpretability with a drop in predictive performance? In some cases, you do not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good. But in other cases, knowing the ‘why’ can help you learn more about the problem, the data and the reason why a model might fail. Some models may not require explanations because they are used in a low-risk environment, meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition). The need for interpretability arises from an incompleteness in problem formalization, which means that for certain problems or tasks it is not enough to get the prediction (the what). The model must also explain how it came to the prediction (the why), because a correct prediction only partially solves your original problem. The following reasons drive the demand for interpretability and explanations.\n",
    "\n",
    "If you can ensure that the machine learning model can explain decisions, you can also check the following traits more easily:\n",
    "* **Fairness**: Ensuring that predictions are unbiased and do not implicitly or explicitly discriminate against protected groups. An interpretable model can tell you why it has decided that a certain person should not get a loan, and it becomes easier for a human to judge whether the decision is based on a learned demographic (e.g. racial) bias.\n",
    "* **Privacy**: Ensuring that sensitive information in the data is protected.\n",
    "* **Reliability or Robustness**: Ensuring that small changes in the input do not lead to large changes in the prediction.\n",
    "* **Causality**: Check that only causal relationships are picked up.\n",
    "* **Trust**: It is easier for humans to trust a system that explains its decisions compared to a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear (Logistic) Regression\n",
    "\n",
    "Linear models can be used to model the dependence of a regression target y on some features x. The learned relationships are linear and can be written for a single instance i as follows:\n",
    "$$y = \\beta_0 + \\beta_1*x_1 + ... + \\beta_n*x_n + e$$\n",
    "\n",
    "A solution for classification is logistic regression. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. The logistic function is defined as:\n",
    "$$ logistic(\\eta) = \\frac{1}{1 + exp(-\\eta)}$$\n",
    "\n",
    "Interpetation of odds (probability of event divided by probability of no event):\n",
    "$$ \\frac{odds_{x_j+1}}{odds_{x_j}} = exp(\\beta_j) $$\n",
    "\n",
    "A change in a feature by one unit changes the odds ratio (multiplicative) by a factor of $exp(\\beta_j)$. We could also interpret it this way: а change in $x_j$ by one unit increases the log odds ratio by the value of the corresponding weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_titanic.csv\")\n",
    "test = pd.read_csv(\"../data/test_titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Survived\"\n",
    "y = train[TARGET].values\n",
    "train = train.drop(TARGET, axis = 1)\n",
    "features = list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=0.1)\n",
    "log_reg.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{feature}: {coef:.2f}\" for coef, feature in zip(log_reg.coef_[0], features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(1+np.exp(sum(train.iloc[0]*log_reg.coef_[0]) + log_reg.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict_proba([train.iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age odds\n",
    "np.exp(-0.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train.iloc[0].copy()\n",
    "temp[\"Age\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(1+np.exp(sum(train.iloc[0]*log_reg.coef_[0]) + log_reg.intercept_)*np.exp(log_reg.coef_[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict_proba([temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to get logistic regression coefficients and their std and p-values.](https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression)\n",
    "\n",
    "But we can't understand what feature is the most important. First of all, we need to scale all features, retrain a model and absolute coefficients values will describe feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=0.1)\n",
    "log_reg.fit(train_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{feature}: {coef:.2f}\" for coef, feature in zip(log_reg.coef_[0], features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "The interpretation is simple: Starting from the root node, you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by ‘AND’.\n",
    "\n",
    "Template: If feature x is smaller/bigger than threshold AND … then the predicted outcome is the mean value of y of the instances in that node.\n",
    "\n",
    "**Feature importance**\n",
    "\n",
    "The overall importance of a feature in a decision tree can be computed in the following way: Go through all the splits for which the feature was used and measure how much it has reduced the variance or [Gini index](https://blog.quantinsti.com/gini-index/) compared to the parent node. The sum of all importances is scaled to 100. This means that each importance can be interpreted as share of the overall model importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(tree, features):\n",
    "    export_graphviz(tree, out_file='tree.dot', \n",
    "                feature_names = features,\n",
    "                class_names = [\"Not survived\", \"Survived\"],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(dt, features)\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=-1, )\n",
    "rf.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(rf.estimators_[0], features)\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree(rf.estimators_[1], features)\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(feature, round(score,2)) for score, feature in zip(rf.feature_importances_, features)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #default\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 4,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"verbosity\": 0,\n",
    "    \"nthread\": 4,\n",
    "    \"random_seed\": 1,\n",
    "    \"eval_metric\": \"auc\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb = xgb.DMatrix(train, y)\n",
    "xgb_model = xgb.train(parameters, train_xgb, num_boost_round=10, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [70, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xgb_model, num_trees=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xgb_model, num_trees=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "xgb.plot_importance(xgb_model, );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget about [xgbfir](https://github.com/limexp/xgbfir). \n",
    "\n",
    "## Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #default\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_threads\": 10,\n",
    "    \"metric\": \"auc\",\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    #regularization\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"min_data_in_leaf\": 15\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb = lgb.Dataset(train, y)\n",
    "lgb_model = lgb.train(parameters, train_lgb, num_boost_round=10, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [70, 50]\n",
    "lgb.plot_tree(lgb_model, tree_index=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "lgb.plot_importance(lgb_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as ctb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"iterations\": 1000,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"random_seed\": 42,\n",
    "    \"od_wait\": 30,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"thread_count\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_data = ctb.Pool(train, y)\n",
    "ctb_model = ctb.train(ctb_data, parameters, iterations=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb_model.plot_tree(0, ctb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features and visualize how predictions will be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_variations = ctb_model.plot_predictions(test.iloc[:2], [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis\n",
    "\n",
    "![image](https://miro.medium.com/max/2560/1*XrQOPzH1aaaUQbubhnDURA.png)\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "* Remove unnecessary features to simplify the model and reduce training/prediction time\n",
    "* Get the most influential feature for your target value and manipulate them for business gains (eg: healthcare providers want to identify what factors are driving each patient’s risk of some disease so they can directly address those risk factors with targeted medicines)\n",
    "\n",
    "Different feature importance metrics:\n",
    "- PredictionValuesChange: calculate score for every feature.\n",
    "- LossFunctionChange: calculate score for every feature by loss (recommended to use for ranking metrics)\n",
    "- FeatureImportance \n",
    "- ShapValues: calculate SHAP Values for every object.\n",
    "- Interaction: calculate pairwise score between every feature (similar to xgbfir)\n",
    "- PredictionDiff: calculate most important features explaining difference in predictions for a pair of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(feature, round(score,2)) for score, feature in zip(ctb_model.feature_importances_, features)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctb_model.get_feature_importance(ctb_data, \"Interaction\")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Importance\n",
    "\n",
    "* Remove the most useless training objects from the training data\n",
    "* Prioritize a batch of new objects for labeling based on which ones are expected to be the most “helpful”, akin to active learning\n",
    "\n",
    "With this functionality, you can calculate the impact of each object on the optimized metric for test data. \n",
    "* Positive values reflect that the optimized metric increases\n",
    "* Negative values reflect that the optimized metric decreases. \n",
    "\n",
    "This method is an implementation of the approach described in [this paper](https://arxiv.org/abs/1802.06640). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ctb_model.get_object_importance(ctb_data, ctb_data, update_method=\"AllPoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots per feature\n",
    "\n",
    "With this feature, we will be able to visualize how the algorithm is splitting the data for each feature and look at feature-specific statistics. More specifically we will be able to see:\n",
    "\n",
    "* Average target (label) value in the bucket.\n",
    "* Average prediction in the bucket.\n",
    "* Number of objects in the bucket.\n",
    "* Average predictions on varying values of the feature.\n",
    "    \n",
    "    To calculate it, the value of the feature is successively changed to fall into every bucket for every input object. The value for a bucket on the graph is calculated as the average for all objects when their feature values are changed to fall into this bucket.\n",
    "    \n",
    "*This plot will give us information like how uniform our splitting is (we don’t want all the objects to go in one bin), whether our predictions are close to target (blue and orange line), the red line will tell us how sensitive our predictions are wrt a feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stats = ctb_model.calc_feature_statistics(ctb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Values\n",
    "\n",
    "A prediction can be explained by assuming that each feature value of the instance is a “player” in a game where the prediction is the payout. Shapley values – a method from coalitional game theory – tells us how to fairly distribute the “payout” among the features.\n",
    "\n",
    "\n",
    "**The Shapley value is the average marginal contribution of a feature value across all possible coalitions.**\n",
    "\n",
    "The feature contributions must add up to the difference of prediction for $x$ and the average.\n",
    "\n",
    "$$\\sum_{j=1}^{p}\\phi_j = f(x) - E_X(f(X))$$\n",
    "\n",
    "More details [here](https://christophm.github.io/interpretable-ml-book/shapley.html#general-idea).\n",
    "\n",
    "**Intuition**\n",
    "\n",
    "An intuitive way to understand the Shapley value is the following illustration: The feature values enter a room in random order. All feature values in the room participate in the game (= contribute to the prediction). The Shapley value of a feature value is the average change in the prediction that the coalition already in the room receives when the feature value joins them.\n",
    "\n",
    "**The interpretation of the Shapley value is: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.**\n",
    "\n",
    "\n",
    "## Shap\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a method to explain individual predictions. SHAP is based on the game theoretically optimal Shapley Values. \n",
    "\n",
    "The goal of SHAP is to explain the prediction of an instance $x$ by computing the contribution of each feature to the prediction.\n",
    "\n",
    "### TreeSHAP\n",
    "\n",
    "TreeSHAP is a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees.\n",
    "\n",
    "Algorithm:\n",
    "* If we conditioned on all features – then the prediction from the node in which the instance $x$ falls would be the expected prediction.\n",
    "* If we did no condition on any feature – we would use the weighted average of predictions of all terminal nodes.\n",
    "* If use some, but not all, features, we ignore predictions of unreachable nodes. From the remaining terminal nodes, we average the predictions weighted by node sizes (i.e. number of training samples in that node). The mean of the remaining terminal nodes, weighted by the number of instances per node, is the expected prediction for $x$ given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(lgb_model, train, model_output=\"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(train)\n",
    "shap.force_plot(explainer.expected_value, shap_values[10,:], train.iloc[10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[:10], train.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(explainer.expected_value, shap_values[:5, :], feature_names=list(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Title\", shap_values, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lime (Local Surrogate)\n",
    "\n",
    "Local surrogate models are interpretable models that are used to explain individual predictions of black box machine learning models. [Local interpretable model-agnostic explanations (LIME)](https://arxiv.org/abs/1602.04938) is a paper in which the authors propose a concrete implementation of local surrogate models. Surrogate models are trained to approximate the predictions of the underlying black box model. Instead of training a global surrogate model, LIME focuses on training local surrogate models to explain individual predictions.\n",
    "\n",
    "The idea is quite intuitive:\n",
    "1. You need just train data (without labels). The goal is to understand why the machine learning model made a certain prediction.\n",
    "2. LIME generates a new dataset consisting of permuted samples and the corresponding predictions of the black box model.\n",
    "3. On this new dataset LIME then trains an interpretable model (regression or dicision tree), which is weighted by the proximity of the sampled instances to the instance of interest. The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation.\n",
    "\n",
    "Mathematically, local surrogate models with interpretability constraint can be expressed as follows:\n",
    "$$explained = arg min L(f, g, \\pi_x) + \\Omega(g)$$\n",
    "The explanation model for instance x is the model $g$ (e.g. linear regression model) that minimizes loss $L$ (e.g. mean squared error), which measures how close the explanation is to the prediction of the original model f (e.g. an xgboost model), while the model complexity $\\Omega(g)$ is kept low (e.g. prefer fewer features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5 import show_weights, explain_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_weights(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_prediction(xgb_model, train.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main source** – [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projector_course",
   "language": "python",
   "name": "projector_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
